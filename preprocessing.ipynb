{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\emre.ozan_icarew\n",
      "[nltk_data]     eb.LAT5520GKJT4B3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\emre.ozan_icareweb\n",
      "[nltk_data]     .LAT5520GKJT4B3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\emre.ozan_icareweb.L\n",
      "[nltk_data]     AT5520GKJT4B3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\emre.ozan_icareweb\n",
      "[nltk_data]     .LAT5520GKJT4B3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\emre.ozan_icareweb.L\n",
      "[nltk_data]     AT5520GKJT4B3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "english_stops = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\nlp_project\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from .autonotebook import tqdm as notebook_tqdm\n",
    "import spacy\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_md model\n",
    "# !python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_id', 'location', 'title', 'organisation', 'description', 'specific_skills', 'sector_skills'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('data/job_description_response.json')\n",
    "job_description = json.load(f)\n",
    "job_description[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The two former companies AAAAAAA and AAAAAAA have merged their processes and software for AAAAAAAA into 1 process in 1 AAAAAAA system. In order to optimize our systems and implement a number of requested legal changes, we are looking for an analyst to lead and support the Premiums project that has been set up. Together with the project leader and fellow analysts, you ensure a successful implementation of these projects.\\nAs a functional analyst, you ensure high-quality and coherent processes by analyzing new and changed processes, initiating proposals for adaptation and by optimizing in the context of an integration of these processes in the existing process structure\\no Together with business and ICT you make an analysis of the proposed improvements and new needs.\\no Write out use cases and discuss them with users and ICT. These use cases are the basis for ICT developments, extending UAT testing and drawing up training and user manuals\\no Supervising testing and testing yourself\\no Providing training or supporting trainers\\no Training key users for further maintenance and testing of the S4Hana system\\no Developing processes and drawing up process documentation in collaboration with business and ICT'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_description[0][\"description\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs = pd.read_csv(\"data/resumes_copy.csv\")\n",
    "cvs.rename(columns={\" resume_text\": \"resume_text\"}, inplace=True)\n",
    "resume = cvs.loc[22, \"resume_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Contact AAAAAAAAAAAAAAA Mobile AAAAAAAAAAAAAAAAAAA AAAAAAAAAAAAAAAAAAAA AAAAAAAAAAAA AAAAAAAA julliescom Company Top Skills IT Service Management Project Management Business Process Management Languages German English Certifications Organizational Behaviour Change Management ADKAR® Model AAAAAAAAAAAAA Project Program Interim Service People Organizational Change Management ADKAR Quality Audit Outsourcing AAAAAAAA Region Summary Competences Expertise domains • Entrepreneur • Project Management ITSM AAAAAAAAAA • Organizational Change Management OCM ADKAR® Model • IT Service Management AAAA V3 • Businessprocess IT process improvements • AAAAAAAAAAAAAAAAAAA Myers Briggs Type Indicator MBTI Type ENTJ Getting things done dream big know how to have fun Social contribution to the society Participate as buddy in the fight against poverty andor the disadvantaged situation Together with AAA AAAAAAAAAAAAAAAAAAA a ten year demonstration project Experience AAAAAAAAAA bv Business Owner 2006 Present 14 years AAAAAAA Entrepreneur Service Manager Process optimisation Organisational Change Management Agile ProgramProject Management Innovator Interior and furniture design Ypto Expert IT Service Management – Business Process Analyst July 2019 November 2019 5 months AAAAAAAA Page 1 of 6 Process design full assessment of delivered IITL processes and implementation in AAAAAAAAAA define and rollout a remediation task force Create process policies Major Incident SOP definition and pragmatic implementation Recommendation ITSM implementation in AAAAAAAAAA Outsourcing support coordination activities for AAAAAAAAAA defects and enhancements with the Service AAAAAAAA Organisational Change Management work together and provide help to the AAAA change management team Document Lifecycle Management proposed a full blown MS Share Point for document life cycle management Training suggested training improvements and provided new content review training content of an external provider AAAAAAAAAA Transition Transfomation outsourcing AAAA expert February 2019 June 2019 5 months AAAAAAAAAAAAAAAAAAAAA Transition support of the incumbent to new Service AAAAAAAA transformation of AAAA processes and ITSM integration of tools Organisational Change Management support FOD Beleid en AAAAAAAAAAAAA Service Manager October 2018 February 2019 5 months AAAAAAAAAAAAAAAAAAAAA Shutdown of the AAAAAAA government like FOD AAAA resulted in zero project budget Contracts not being extended onboarding new Service Consumers optimisation process communication forms AAAA Portal enhancement of the supporting tool AAAAAAAAAA improve collaboration across teams AAAAAAA Authentic Source definition AAA Armen Te Kort AAAAAAAAAAA Buddy June 2017 January 2019 1 year 8 months AAAAAAA Area AAAAAAA goal of this AAA project “Our generation will stop the disadvantaged situation on poverty” by this demonstration project in showing that we can reach up to 5000 people in a disadvantaged situation of poverty in recovering their self Page 2 of 6 esteem so they can regain the direction of their life in a sustainable manner From the project will grow a basic movement that creates a social basis for the fundamental solutions AAAAAAAAA Transition Transformation Service Contract Problem management May 2018 September 2018 5 months AAAAAAAAAAAAAAAAAAAAA Project Management Transition Transformation Service track AAAA processes incident problem change Request Fulfilment Capacity Asset Configuration Reporting Governance AAA’s Service Catalogue Transition track AD ADFS MPS Exchange 2016 Enterprise Vault Skype for Business File services FDS Print Services Monitoring Transformation track Data Centre Backup Monitoring O365 Skype for Business OneDrive Citrix Intune Faxserver SNOW AAAAAAAAAA SCCM Contract track Rollout the Formal Problem Management track Identify Service improvements on Incident management including quick wins AAAAAAAAAAAAAAAAA IT Service Management expertise AAAAAAAAAA Asset Configuration August 2017 April 2018 9 months AAAAAAAAAAAAAAAAAAA Leverage operational experience and knowledge in setting up from scratch an Asset configuration process ITSM AAAA V3 NEW process policy CMDB CI Category attribute identification ADDED Functionality for Distributed CI management automated Discovery and relationship setting automated CI owner assignment review process facilitated PIR Change Process to support a global company served by many external Managed Service AAAAAAAAs With the focus on MANAGED AAAAAAAAAA cloud AAAAAAAAAA Discovery AAAAAAA Training AAAAAAAAAAAAAAAAAAAAAAAA Service Management expert Application Project Management October 2015 October 2016 1 year 1 month AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA Page 3 of 6 Project and Change Manager March 2015 October 2015 8 months AAAAAAAAAAAAAAAAAAAAA • Project management • ITSM AAAA V3 Review improve service delivery processes and documents Improve Changebid process Operational Change Manager Transformation Tools Methodologies Omnitracker MS PowerPoint AAAAAAAAAAAAAAAAAAAA AAAAAAA Service Request Project Manager April 2014 January 2015 10 months AAAAAAAAAAAAAAA • Project  management multiple internal ICT OPS projects in parallel Tools Methodologies Clarity MS SharePoint Ms PowerPoint ALM Application Lifecycle Management • ITSM Identified 50 Service Improvements objective increase CMMI level Operational Change Management Design and build the Service Catalogue framework in MS SharePoint Plan and Coordination Infrastructure release deployment Tools Methodologies MS Visio AAAA V3 USD Unicenter Service Desk AVAYA desktop CMMI EUROCONTROL Service Management expert Document live cycle management Organizational Change December 2012 March 2014 1 year 4 months Diegem AAAAAAAAAAAAAAAA Johnson Johnson Agile Infrastructure Project Management May 2012 December 2012 8 months AAAAAAA HP Agile Migration Project Management Organizational Change 2011 2012 1 year AAAAAAAAAAAAAAAAAAAAA Belgacom Service Management expert Communication Manager Page 4 of 6 September 2010 April 2011 8 months AAAAAAAAAAAAAAAAAAAAA Victorybe Relationship Manager Business Development Germany 2010 October 2010 less than a year AAAAAAA Germany Torry Harris Business Solutions Service Management expert April 2009 January 2010 10 months Luxembourg Vodafone Group Services 9 months Service Management expert and Agile Project Manager September 2008 January 2009 5 months Duesseldorf IT Service Management Change Manager May 2008 September 2008 5 months Germany Telindus Sourcing a Belgacom ICT Service Management expert and Agile Project Manager August 2007 May 2008 10 months AAAAAAA Star Consulting Group Ltd Lean Management June 2007 August 2007 3 months The Hague Area Netherlands IIB ACCREDITED CONSULTANCY PRACTICES Business support Management May 2006 May 2007 1 year 1 month AAAAAAA American Management Systems Inc CGI Page 5 of 6 Service Management expert Agile Project Manager Business Consultant October 1994 April 2006 11 years 7 months Germany the Netherlands Poland AAAAAAA Buga IT Project Manager IT Manager November 1988 September 1994 5 years 11 months AAAAAAA Area AAAAAAA CIMHardy ICT Consultant January 1985 October 1988 3 years 10 months Gent Area AAAAAAA Education Thomas More Kempen BSc Information Technology · 1981 1983 AAAAAAAA Prince2 Project Management AAAAAAAA Organizational Change ADKARProsci Duesseldorf AAAA V2 Foundation Luxembourg AAAA V3 Foundation Bridging Page 6 of 6 '"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvs.resume_text[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a-) Clean Resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes = cvs.resume_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Contact Details AAAAAAAAAAAAAAAAAAAAA wwwlinkedincomin AAAAAAAAAAAAAAAAA LinkedIn Key Skills Business Analysis Functional Analysis Requirements Analysis Languages Dutch Native or Bilingual English Full Professional French Limited Working Russian Native or Bilingual Certifications Ultimate Guide to User Stories Professional Scrum Master I Bachelors Degree Applied Computer Science Business Solutions Business Analysis Foundations ITIL 4 Foundation Certificate in IT Service AAAAAAAAAAAAAAAAAAAAAAA Freelance Business Functional Analyst Rotselaar Experience Agency for Nature and Forest Business And Functional Analyst July 2022 Present 1 month AAAAAAA AAAAAAA Consulting BV Professional Freelancer July 2021 Present 1 year 1 month AAAAAAAAAAA 6 years Business Analyst January 2020 July 2022 2 years 7 months AAAAAAA Area Functional Analyst July 2019 July 2022 3 years 1 month AAAAAAA Area Technical Analyst January 2018 July 2019 1 year 7 months AAAAAAA Area Case360 Developer August 2016 July 2019 3 years AAAAAAA Area Devoteam Consultant August 2016 July 2021 5 years AAAAAAA and environs Belgium Telenet Fulfillment Coordinator August 2015 July 2016 1 year Page 1 of 2 AAAAAAAA AAAAAAA Internship Business Control IT March 2014 June 2014 4 months Functional analysis and development of an ECM system on the SalesForce platform Education AAAAAAAAAAAAAAAAA Bachelors degree Applied Informatics Business Solutions Page 2 or 2 '"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove repeating letter, page, and page numbers and translate the text into English\n",
    "patern_repetetive = r\"(.)\\1{2,}\"\n",
    "pattern_page = r\"Page \\d+ of \\d+\"\n",
    "for i, resume in enumerate(resumes):\n",
    "    resume = re.sub(patern_repetetive, \"\", resume)\n",
    "    resumes[i] = re.sub(pattern_page, \"\", resume)\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens =[word_tokenize(article) for article in resumes]\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens = [[t.lower() for t in token] for token in tokens]\n",
    "\n",
    "# remove repeating letters\n",
    "\n",
    "# Retain alphabetic and numerical words: alpha_only\n",
    "alpha_only_list = [[t for t in lower_token if t.isalpha() or t.isdigit()] for lower_token in lower_tokens]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [[t for t in alpha_only if t not in english_stops] for alpha_only in alpha_only_list]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "articles_lemmatized = [[wordnet_lemmatizer.lemmatize(t) for t in no_stop] for no_stop in no_stops]  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b-) Name Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_lemmatized_str = [\" \".join(article) for article in articles_lemmatized]\n",
    "cvs[\"resume_cleaned\"] = articles_lemmatized_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import countries list\n",
    "with open('data/countries.txt', 'r') as file:\n",
    "    ex = 'start'\n",
    "    countries = []\n",
    "    c = file.readline()\n",
    "    while ex=='start':\n",
    "        c = c.replace(\"'\", '')\n",
    "        c = c.split(',')[0]\n",
    "        c = c.strip()\n",
    "        countries.append(c)\n",
    "        c = file.readline()\n",
    "        if c=='':\n",
    "            ex='end'\n",
    "countries = [country.lower() for country in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_md model\n",
    "# python -m spacy download en_core_web_md\n",
    "# nlp = spacy.load('en_core_web_md')\n",
    "cvs[\"locations\"] = None\n",
    "for i, row in cvs.iterrows():\n",
    "    article = row[\"resume_cleaned\"]\n",
    "    doc = nlp(article)\n",
    "\n",
    "    matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "    # Create pattern Doc objects and add them to the matcher\n",
    "    # This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "    patterns = list(nlp.pipe(countries))\n",
    "    matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "    # Call the matcher on the test document and print the result\n",
    "    matches = matcher(doc)\n",
    "    matches_list = [str(doc[start:end]) for match_id, start, end in matches]\n",
    "    cvs.loc[i, \"locations\"] = matches_list\n",
    "    # if \"belgium\" in matches_list:\n",
    "    #     print(\"yes\")\n",
    "    #print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the en_core_web_md model\n",
    "# !python -m spacy download en_core_web_md\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b-1) Filter Locations in CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs[\"locations\"] = None\n",
    "for i, row in cvs.iterrows():\n",
    "    doc = nlp(row[\"resume_cleaned\"])\n",
    "    # Filter location entities\n",
    "    cvs.loc[i, \"locations\"] = [ent.text for ent in doc.ents if ent.label_ in ['GPE', 'LOC']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b-2) Filter Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs[\"languages\"] = None\n",
    "for i, row in cvs.iterrows():\n",
    "    doc = nlp(row[\"resume_cleaned\"])\n",
    "    # Filter location entities\n",
    "    cvs.loc[i, \"languages\"] = [ent.text for ent in doc.ents if ent.label_ in ['LANGUAGE', 'NORP']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b-3) Filter Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contact detail linkedincomin linkedin key skill business analysis functional analysis requirement analysis language dutch native bilingual english full professional french limited working russian native bilingual certification ultimate guide user story professional scrum master bachelor degree applied computer science business solution business analysis foundation itil 4 foundation certificate service freelance business functional analyst rotselaar experience agency nature forest business functional analyst july 2022 present 1 month consulting bv professional freelancer july 2021 present 1 year 1 month 6 year business analyst january 2020 july 2022 2 year 7 month area functional analyst july 2019 july 2022 3 year 1 month area technical analyst january 2018 july 2019 1 year 7 month area developer august 2016 july 2019 3 year area devoteam consultant august 2016 july 2021 5 year environs belgium telenet fulfillment coordinator august 2015 july 2016 1 year internship business control march 2014 june 2014 4 month functional analysis development ecm system salesforce platform education bachelor degree applied informatics business solution page 2 2'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvs.resume_cleaned[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_year = r\"\\b\\d{4}\\b\"\n",
    "pattern_year_and_month = r\"(\\d+)\\s*year(s?)|(\\d+)\\s*month(s?)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs[\"experiences\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs[\"experiences\"] = None\n",
    "for i, row in cvs.iterrows():\n",
    "    article = row[\"resume_cleaned\"]\n",
    "    doc = nlp(article)\n",
    "    experiences = []\n",
    "    start_index = 0\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'DATE':\n",
    "            if re.search(pattern_year, ent.text):\n",
    "                # print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "                if start_index == 0:\n",
    "                    start_index = ent.start_char\n",
    "                    continue\n",
    "                # print(ent.text)\n",
    "                # print(start_index, ent.start_char)\n",
    "                experience_text = article[start_index:ent.start_char]\n",
    "                # print(experience_text)\n",
    "                # match = re.search(pattern_year_and_month, first_article[start_index:ent.start_char])\n",
    "                matches = re.findall(pattern_year_and_month, experience_text)\n",
    "                # if match:\n",
    "                experience_time = 0\n",
    "                year = 0\n",
    "                month = 0\n",
    "                for match in matches:\n",
    "                    if match[0]:\n",
    "                        year = int(match[0])\n",
    "                        experience_time += year\n",
    "                    if match[2]:\n",
    "                        month = int(match[2])\n",
    "                        experience_time += month/12\n",
    "                experience_text = re.sub(pattern_year_and_month, \"\", experience_text) \n",
    "                if experience_time > 0:\n",
    "                    experience = {}\n",
    "                    experience[\"experience_time\"] = round(experience_time, 3)\n",
    "                    # print(round(experience_time, 3))\n",
    "                    sub_doc = nlp(experience_text)\n",
    "                    for sub_ent in sub_doc.ents:\n",
    "                        if sub_ent.label_ == 'DATE':\n",
    "                            remove_pattern = sub_ent.text\n",
    "                            experience_text = re.sub(remove_pattern, \"\", experience_text) \n",
    "                    if experience_text:\n",
    "                        experience[\"experience_text\"] = experience_text  \n",
    "                        experiences.append(experience)    \n",
    "                start_index = ent.start_char\n",
    "    if experiences:\n",
    "        cvs.at[i, \"experiences\"] = experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvs.loc[21][\"experiences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'resume_text', 'resume_cleaned', 'locations', 'languages',\n",
       "       'experiences'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvs.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'george_boole'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvs.loc[21][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    TELENETBE  BELINKEDINCOMIN  SPIRITWORKSBE PERSONAL DATA Date of birth  AA  Address   Trilingual Dutch native French 2nd language English 3rd language SKILLS People Management Member Team Coaching EXPERIENCE FUNCTIONAL ANALYST ACV CSC  now Functional analysis and SEO of new corporate website using  CONSULTANT STRAND ASSOCIATES 102016 –  Client  Task Project Management INTERNET INTRANET CONSULTANT FEDICT 112015 –  Client  Task Project Management WEB CONSULTANT CHR ONOS  – 102015 Functional and Business Analysis EMARKETER ATEL  – 122014 EMarketing Strategy EBUSINESS EXPERT OZ  –  EMarketing Strategy WEB CONSULTANT – AUSY DATA FLOW  –  Client  Task Functional and Usability Analysis WEB PROJECT MANAGER E2E SOLUTIONS 112008 –  Clients   INTERACTIVE MANAGER TMPADCOMMS  –  Clients HP USG ED S 2 EXPERTISE EMarketing Search Engine Optimization UX Analysis Usability WEB PROJECT MANAGER MOTIONMILL  – 122007 Clients    STUDIO MANAGER C SHARP  – 122004 Clients  Task Project Management ART DIRECTOR UX DE SIGNER YOUNG PARTNERS  –  Clients Navision Microman QUALITY MANAGER RUSH MAGAZINE  –  Team Leader ART DIRECTOR ALLEN INTERNATIONAL LONDON  –  Clients K redietbank Fortis STUDIO MANAGER ANV ERS FINANCE PARIS  –  Team Leader EDUCATION FUNCTIONAL BUSINESS ANALYST The Master Labs Kontich  –  INTEGRATIVE PSYCHOLO GY    –  PRINCE 2 FOUNDATION   –  MASTER INTERNET W EB DEVELOPMENT   –  APPLIED GRAPHICS St    –  VOLUNTEER EXPERIENCE YAR  2011 2014 This program Youth at Risk allows young people with a criminal past to feel that they are solely responsible for the choices they make in life Making conscious choices is the first step to take your own life in your hands '"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvs.loc[21][\"resume_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('February', '2017')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is an example text. February 2017 is the date we are interested in.\"\n",
    "\n",
    "pattern = r\"\\b([A-Za-z]+)\\s(\\d{4})\\b\"\n",
    "\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_month_year = r\"\\b(\\d{2}) (\\d{4})\\b\"\n",
    "pattern_all_digit = r\"\\d\"\n",
    "pattern_month_alfa_year = r\"\\b[A-Za-z]{3}\\s\\d{4}\\b\"\n",
    "month_mapping = {\n",
    "    'jan': 1,\n",
    "    'feb': 2,\n",
    "    'mar': 3,\n",
    "    'apr': 4,\n",
    "    'may': 5,\n",
    "    'jun': 6,\n",
    "    'jul': 7,\n",
    "    'aug': 8,\n",
    "    'sep': 9,\n",
    "    'oct': 10,\n",
    "    'nov': 11,\n",
    "    'dec': 12\n",
    "}\n",
    "pattern_month_long_alfa_year = r\"\\b([A-Za-z]+)\\s(\\d{4})\\b\"\n",
    "long_month_mapping = {\n",
    "    'january': 1,\n",
    "    'february': 2,\n",
    "    'march': 3,\n",
    "    'april': 4,\n",
    "    'may': 5,\n",
    "    'june': 6,\n",
    "    'july': 7,\n",
    "    'august': 8,\n",
    "    'september': 9,\n",
    "    'october': 10,\n",
    "    'november': 11,\n",
    "    'december': 12\n",
    "}\n",
    "pattern_adjacent_month_year = r\"\\b(\\d{2})(\\d{4})\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('10', '2016'), ('11', '2015'), ('10', '2015'), ('12', '2014'), ('11', '2008'), ('12', '2007'), ('12', '2004')]\n",
      "('11', '2015')\n",
      "{'year': 2016, 'month': 10}\n",
      "('10', '2015')\n",
      "{'year': 2015, 'month': 11}\n",
      "('12', '2014')\n",
      "{'year': 2015, 'month': 10}\n",
      "('11', '2008')\n",
      "{'year': 2014, 'month': 12}\n",
      "('12', '2007')\n",
      "{'year': 2008, 'month': 11}\n",
      "('12', '2004')\n",
      "{'year': 2007, 'month': 12}\n"
     ]
    }
   ],
   "source": [
    "article = cvs[\"resume_cleaned\"][21]\n",
    "doc = nlp(article)\n",
    "experiences = []\n",
    "adjacent = {}\n",
    "start_index = 0\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'DATE':\n",
    "        experience_time = 0\n",
    "        year = 0\n",
    "        month = 0\n",
    "        if start_index == 0:\n",
    "            start_index = ent.start_char\n",
    "            continue\n",
    "        experience_text = article[start_index:ent.start_char]\n",
    "        # print(experience_text)\n",
    "        start_index = ent.start_char\n",
    "        if re.search(pattern_year, ent.text):\n",
    "            # print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "            # print(ent.text, ent.label_)\n",
    "            # print(start_index, ent.start_char)\n",
    "            if re.search(pattern_year_and_month, experience_text):\n",
    "                matches = re.findall(pattern_year_and_month, experience_text)\n",
    "                # print(experience_text)\n",
    "                for match in matches:\n",
    "                    if match[0]:\n",
    "                        year = int(match[0])\n",
    "                        experience_time += year\n",
    "                    if match[2]:\n",
    "                        month = int(match[2])\n",
    "                        experience_time += month/12\n",
    "                experience_text = re.sub(pattern_year_and_month, \"\", experience_text)    \n",
    "            elif re.search(pattern_month_year, ent.text):\n",
    "                # print(experience_text)\n",
    "                matches = re.findall(pattern_month_year,  experience_text)\n",
    "                # print(matches)\n",
    "                if len(matches) > 1:\n",
    "                    months = [None] * len(matches)\n",
    "                    years = [None] * len(matches)\n",
    "                    for i, match in enumerate(matches):\n",
    "                        months[i] = match[0]\n",
    "                        years[i] = match[1]\n",
    "                    year = int(years[-1]) - int(years[-2])\n",
    "                    month = int(months[-1]) - int(months[-2])\n",
    "                    experience_time += year\n",
    "                    experience_time += month/12\n",
    "            elif re.search(pattern_month_alfa_year, experience_text):\n",
    "                matches = re.findall(pattern_month_alfa_year,  experience_text)\n",
    "                if len(matches) > 1:\n",
    "                    months = [None] * len(matches)\n",
    "                    years = [None] * len(matches)\n",
    "                    for i, match in enumerate(matches):\n",
    "                        months[i] = match[0:3]\n",
    "                        years[i] = match[-5:]\n",
    "                    months = [month_mapping.get(month_abbreviation) for month_abbreviation in months]\n",
    "                    year = int(years[-1]) - int(years[-2])\n",
    "                    month = int(months[-1]) - int(months[-2])\n",
    "                    experience_time += year\n",
    "                    experience_time += month/12\n",
    "            elif re.search(pattern_month_long_alfa_year, experience_text):\n",
    "                matches = re.findall(pattern_month_long_alfa_year,  experience_text)\n",
    "                if len(matches) > 1:\n",
    "                    months = [None] * len(matches)\n",
    "                    years = [None] * len(matches)\n",
    "                    for i, match in enumerate(matches):\n",
    "                        months[i] = match[0]\n",
    "                        years[i] = match[1]\n",
    "                    months = [long_month_mapping.get(month) for month in months]\n",
    "                    if not None in months:\n",
    "                        year = int(years[-1]) - int(years[-2])\n",
    "                        month = int(months[-1]) - int(months[-2])\n",
    "                        experience_time += year\n",
    "                        experience_time += month/12\n",
    "            elif re.search(pattern_adjacent_month_year, experience_text):\n",
    "                matches = re.findall(pattern_adjacent_month_year,  experience_text)\n",
    "                print(matches)\n",
    "                for i, match in enumerate(matches):\n",
    "                    if i!=0:\n",
    "                        print(match)\n",
    "                        print(adjacent)\n",
    "                        year = adjacent[\"year\"] - int(match[1]) \n",
    "                        month = adjacent[\"month\"] - int(match[0])\n",
    "                        experience_time += year\n",
    "                        experience_time += month/12\n",
    "                    adjacent[\"year\"] = int(match[1])\n",
    "                    adjacent[\"month\"] = int(match[0])\n",
    "        if experience_time > 0:\n",
    "            experience = {}\n",
    "            experience[\"experience_time\"] = round(experience_time, 3)\n",
    "            # print(round(experience_time, 3))\n",
    "            sub_doc = nlp(experience_text)\n",
    "            for sub_ent in sub_doc.ents:\n",
    "                if sub_ent.label_ == 'DATE':\n",
    "                    remove_pattern = sub_ent.text\n",
    "                    # experience_text = re.sub(remove_pattern, \"\", experience_text) \n",
    "                    experience_text = re.sub(pattern_all_digit, \"\", experience_text)\n",
    "            if experience_text:\n",
    "                experience[\"experience_text\"] = experience_text  \n",
    "                experiences.append(experience)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "text = resumes[23]\n",
    "\n",
    "# Load the en_core_web_md model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "doc = nlp(text)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(countries))\n",
    "matcher.add('COUNTRY', None, *patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_lemmatized[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('job_desc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7fe6c830fc9e1307e851b4737afa8b20f38f83a101c3b36e1b261b19ff46c963"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
